{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Buddhist LLM Project - Environment Setup\n",
    "# Part 1: Essential Installations\n",
    "!pip install -q torch transformers datasets accelerate wandb anthropic \n",
    "!pip install -q huggingface_hub  # For model access and management\n",
    "\n",
    "# Part 2: Core Imports\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from google.colab import drive\n",
    "import anthropic\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Part 3: Configure Environment\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define project directories\n",
    "base_dir = '/content/drive/MyDrive/Buddhist_LLM'\n",
    "project_structure = {\n",
    "    'data': {\n",
    "        'raw_texts': 'Original Buddhist texts',\n",
    "        'processed': 'Processed training data'\n",
    "    },\n",
    "    'models': {\n",
    "        'checkpoints': 'Training checkpoints',\n",
    "        'fine_tuned': 'Final models'\n",
    "    },\n",
    "    'logs': 'Training and evaluation logs'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for parent, subdirs in project_structure.items():\n",
    "    if isinstance(subdirs, dict):\n",
    "        for subdir, description in subdirs.items():\n",
    "            full_path = f\"{base_dir}/{parent}/{subdir}\"\n",
    "            os.makedirs(full_path, exist_ok=True)\n",
    "            print(f\"✓ {full_path} - {description}\")\n",
    "    else:\n",
    "        full_path = f\"{base_dir}/{parent}\"\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        print(f\"✓ {full_path} - {subdirs}\")\n",
    "\n",
    "# Part 4: Environment Checks\n",
    "print(\"\\nEnvironment Status:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check GPU\n",
    "print(\"\\n1. GPU Configuration:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✓ Memory Available: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠ No GPU available. Please enable GPU in Runtime settings.\")\n",
    "\n",
    "# Check PyTorch\n",
    "print(\"\\n2. PyTorch Version:\", torch.__version__)\n",
    "\n",
    "# Check Transformers\n",
    "from transformers import __version__ as transformers_version\n",
    "print(\"3. Transformers Version:\", transformers_version)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
